{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568c322f-7f77-426f-b4e6-4368fd1436ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b0c140-d96a-494d-b995-3412a1c84891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text  Score\n",
      "0     I have bought several of the Vitality canned d...      5\n",
      "1     Product arrived labeled as Jumbo Salted Peanut...      1\n",
      "2     This is a confection that has been around a fe...      4\n",
      "3     If you are looking for the secret ingredient i...      2\n",
      "4     Great taffy at a great price.  There was a wid...      5\n",
      "...                                                 ...    ...\n",
      "9995  we switched from the advance similac to the or...      1\n",
      "9996  Like the bad reviews say, the organic formula ...      5\n",
      "9997  I wanted to solely breastfeed but was unable t...      5\n",
      "9998  i love the fact that i can get this delieved t...      5\n",
      "9999  We have a 7 week old... He had gas and constip...      4\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "                                                   Text  Score\n",
      "0     I have bought several of the Vitality canned d...      1\n",
      "1     Product arrived labeled as Jumbo Salted Peanut...      0\n",
      "2     This is a confection that has been around a fe...      1\n",
      "3     If you are looking for the secret ingredient i...      0\n",
      "4     Great taffy at a great price.  There was a wid...      1\n",
      "...                                                 ...    ...\n",
      "9995  we switched from the advance similac to the or...      0\n",
      "9996  Like the bad reviews say, the organic formula ...      1\n",
      "9997  I wanted to solely breastfeed but was unable t...      1\n",
      "9998  i love the fact that i can get this delieved t...      1\n",
      "9999  We have a 7 week old... He had gas and constip...      1\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "                                                   Text  Score\n",
      "0     I have bought several of the Vitality canned d...      1\n",
      "1     Product arrived labeled as Jumbo Salted Peanut...      0\n",
      "2     This is a confection that has been around a fe...      1\n",
      "3     If you are looking for the secret ingredient i...      0\n",
      "4     Great taffy at a great price There was a wide ...      1\n",
      "...                                                 ...    ...\n",
      "9995  we switched from the advance similac to the or...      0\n",
      "9996  Like the bad reviews say the organic formula c...      1\n",
      "9997  I wanted to solely breastfeed but was unable t...      1\n",
      "9998  i love the fact that i can get this delieved t...      1\n",
      "9999  We have a 7 week old He had gas and constipati...      1\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "                                                   Text  Score\n",
      "0     [I, have, bought, several, of, the, Vitality, ...      1\n",
      "1     [Product, arrived, labeled, as, Jumbo, Salted,...      0\n",
      "2     [This, is, a, confection, that, has, been, aro...      1\n",
      "3     [If, you, are, looking, for, the, secret, ingr...      0\n",
      "4     [Great, taffy, at, a, great, price, There, was...      1\n",
      "...                                                 ...    ...\n",
      "9995  [we, switched, from, the, advance, similac, to...      0\n",
      "9996  [Like, the, bad, reviews, say, the, organic, f...      1\n",
      "9997  [I, wanted, to, solely, breastfeed, but, was, ...      1\n",
      "9998  [i, love, the, fact, that, i, can, get, this, ...      1\n",
      "9999  [We, have, a, 7, week, old, He, had, gas, and,...      1\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Data processing\n",
    "# 1.1 Read CSV file, select the first 10,000 records and keep 'Text' & 'Score'\n",
    "file_path = 'Reviews.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df_filtered = df[['Text', 'Score']].head(10000)\n",
    "df_id = df['Id'].head(10000)\n",
    "print(df_filtered)\n",
    "\n",
    "# Convert Score>=4 to Score=1, and convert others to Score=0 (1: positive 0: negative)\n",
    "df_filtered['Score'] = df_filtered['Score'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "print(df_filtered)\n",
    "\n",
    "# Remove punctuation and extra spaces from the 'Text' column\n",
    "df_filtered['Text'] = df_filtered['Text'].str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "df_filtered['Text'] = df_filtered['Text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "print(df_filtered)\n",
    "\n",
    "# Split the text in the 'Text' column using the delimiter\n",
    "df_filtered['Text'] = df_filtered['Text'].str.split(' ')\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033097f1-8440-431a-a46d-735da2747f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text  Score\n",
      "0     bought vitality canned dog food products good ...      1\n",
      "1     product arrived labeled jumbo salted peanutsth...      0\n",
      "2     confection centuries light pillowy citrus gela...      1\n",
      "3     looking secret ingredient robitussin believe g...      0\n",
      "4     great taffy great price wide assortment yummy ...      1\n",
      "...                                                 ...    ...\n",
      "9995  switched advance similac organic product think...      0\n",
      "9996  like bad reviews say organic formula constipat...      1\n",
      "9997  wanted solely breastfeed unable supplement for...      1\n",
      "9998  love fact delieved house delievy chargeit hard...      1\n",
      "9999  week old gas constipation problems weeks tried...      1\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Remove stop words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "df_filtered['Text'] = df_filtered['Text'].apply(lambda x: ' '.join(vectorizer.build_analyzer()(' '.join(x))))\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66bd87a9-4c07-404c-bea0-96c97a9a8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 4375), ('like', 4268), ('coffee', 3543), ('good', 3411), ('taste', 2929), ('just', 2878), ('great', 2797), ('flavor', 2598), ('product', 2450), ('love', 2028)]\n"
     ]
    }
   ],
   "source": [
    "# Remove frequent words\n",
    "# Use CountVectorizer to calculate word frequencies\n",
    "vectorizer = CountVectorizer()\n",
    "word_counts = vectorizer.fit_transform(df_filtered['Text'])\n",
    "\n",
    "# Sum the word frequencies\n",
    "word_freq = word_counts.sum(axis=0)\n",
    "word_freq = [(word, word_freq[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "\n",
    "# Sort the words by frequency\n",
    "word_freq_sorted = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "print(word_freq_sorted[:10])\n",
    "\n",
    "# Define frequent words (for example, top 10 most frequent words)\n",
    "frequent_words = [word for word, freq in word_freq_sorted[:10]]  # Top 10 frequent words\n",
    "\n",
    "# Remove frequent words from the 'Text' column\n",
    "df_filtered['Text'] = df_filtered['Text'].apply(lambda text: ' '.join([word for word in text.split() if word not in frequent_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93f5ea4-73f0-420e-a95a-7df20e877e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF time: 0.26729893684387207 sec\n",
      "TF-IDF Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Word2Vec time: 6.768906593322754 sec\n",
      "Word2Vec Matrix:\n",
      " [[ 0.02018949  0.03739723 -0.00541242 ... -0.00196284  0.03204571\n",
      "   0.04624848]\n",
      " [-0.03297562  0.03235491 -0.00132849 ...  0.04655     0.08154547\n",
      "   0.0123597 ]\n",
      " [ 0.02657184  0.04558397 -0.00805743 ...  0.00161942  0.06419811\n",
      "   0.01243184]\n",
      " ...\n",
      " [-0.00267181  0.04989159  0.00206788 ...  0.00506341  0.05094728\n",
      "   0.02337295]\n",
      " [ 0.00566839  0.04982306 -0.00472831 ...  0.029323    0.03726533\n",
      "   0.01427644]\n",
      " [ 0.00095775  0.04158614  0.00667466 ...  0.00477131  0.02261127\n",
      "   0.02707624]]\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Text mining preprocessing: Convert the text into vectors, implement TF-IDF and Word2Vec, and compare the results\n",
    "# Implement TF-IDF\n",
    "start_time = time.time()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_filtered['Text'])\n",
    "end_time = time.time()\n",
    "print(f\"TF-IDF time: {end_time - start_time} sec\")\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Implement Word2Vec\n",
    "start_time = time.time()\n",
    "word2vec_model = Word2Vec(df_filtered['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "def average_word_vectors(words, model, vector_size):\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "df_filtered['Word2Vec_Vector'] = df_filtered['Text'].apply(lambda x: average_word_vectors(x, word2vec_model, 100))\n",
    "Word2Vec_vectors = np.array(df_filtered['Word2Vec_Vector'].tolist())\n",
    "end_time = time.time()\n",
    "print(f\"Word2Vec time: {end_time - start_time} sec\")\n",
    "print(\"Word2Vec Matrix:\\n\", Word2Vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624c93ca-9080-447b-b248-98d2f1cb3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Random Forest Accuracy: 0.8250\n",
      "Word2Vec Random Forest Accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "# 2. Modeling: Use Random Forest for classification\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test, idx_train_tfidf, idx_test_tfidf = \\\n",
    "    train_test_split(tfidf_matrix, df_filtered['Score'],df_id, test_size=0.4, random_state=42)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec, idx_train_word2vec, idx_test_word2vec = \\\n",
    "    train_test_split(Word2Vec_vectors, df_filtered['Score'], df_id, test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier_word2vec = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "rf_classifier_word2vec.fit(X_train_word2vec, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_tfidf = rf_classifier_tfidf.predict(X_test_tfidf)\n",
    "y_pred_word2vec = rf_classifier_word2vec.predict(X_test_word2vec)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "print(f\"TF-IDF Random Forest Accuracy: {accuracy_tfidf:.4f}\")\n",
    "accuracy_word2vec = accuracy_score(y_test, y_pred_word2vec)\n",
    "print(f\"Word2Vec Random Forest Accuracy: {accuracy_word2vec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886ed88-ac0f-4f67-919b-51021d17bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluate the model: Perform k-fold cross-validation with k=4 and calculate the accuracy\n",
    "accuracy_scores_tfidf = cross_val_score(rf_classifier_tfidf, tfidf_matrix, df_filtered['Score'], cv=4, scoring='accuracy')\n",
    "accuracy_scores_word2vec = cross_val_score(rf_classifier_word2vec, Word2Vec_vectors, df_filtered['Score'], cv=4, scoring='accuracy')\n",
    "print(f\"TF-IDF Average Accuracy: {accuracy_scores_tfidf.mean():.4f}\")\n",
    "print(f\"Word2Vec Average Accuracy: {accuracy_scores_word2vec.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f054f2-a031-4322-949b-18091cf49b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "idx_test_tfidf_reset = pd.Series(range(1, len(idx_test_tfidf) + 1))\n",
    "idx_test_word2vec_reset = pd.Series(range(1, len(idx_test_word2vec) + 1))\n",
    "\n",
    "submission_tfidf = pd.DataFrame({\n",
    "    'ID': range(1, len(idx_test_tfidf) + 1),\n",
    "    'Score': y_pred_tfidf\n",
    "})\n",
    "submission_word2vec = pd.DataFrame({\n",
    "    'ID': range(1, len(idx_test_word2vec) + 1),\n",
    "    'Score': y_pred_word2vec\n",
    "})\n",
    "\n",
    "submission_tfidf.to_csv('submission_tfidf.csv', index=False)\n",
    "submission_word2vec.to_csv('submission_word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b811a-c742-435d-96a8-d8af4d9affac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
